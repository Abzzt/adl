{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2832 709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\adl\\data_proccessing.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_new = pd.concat([df_new, data_entry[data_entry[l]==1][:300]], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from data_proccessing import valid_loader, train_loader, labels\n",
    "from helper import train, evaluate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)  # Example number of classes\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomAlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomAlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of AlexNet\n",
    "custom_alexnet = CustomAlexNet()\n",
    "custom_alexnet.to(device)\n",
    "print(custom_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import os\n",
    "\n",
    "# class CustomAlexNet(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CustomAlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(4096, num_classes),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# # Set up directories and model\n",
    "# # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# custom_alexnet = CustomAlexNet(num_classes).to(device)  # Adjust num_classes as necessary\n",
    "# print(custom_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001, Epoch: 1, Test Loss: 0.4192, Validation Loss: 0.4054\n",
      "Learning Rate: 0.001, Epoch: 2, Test Loss: 0.3862, Validation Loss: 0.3825\n",
      "Learning Rate: 0.001, Epoch: 3, Test Loss: 0.3840, Validation Loss: 0.4482\n",
      "Learning Rate: 0.001, Epoch: 4, Test Loss: 0.3793, Validation Loss: 0.3957\n",
      "Learning Rate: 0.001, Epoch: 5, Test Loss: 0.3760, Validation Loss: 0.4000\n",
      "Learning Rate: 0.001, Epoch: 6, Test Loss: 0.3722, Validation Loss: 0.3801\n",
      "Learning Rate: 0.001, Epoch: 7, Test Loss: 0.3710, Validation Loss: 0.3690\n",
      "Learning Rate: 0.001, Epoch: 8, Test Loss: 0.3704, Validation Loss: 0.3665\n",
      "Learning Rate: 0.001, Epoch: 9, Test Loss: 0.3700, Validation Loss: 0.3644\n",
      "Learning Rate: 0.001, Epoch: 10, Test Loss: 0.3693, Validation Loss: 0.3630\n",
      "Learning Rate: 0.001, Epoch: 11, Test Loss: 0.3683, Validation Loss: 0.3606\n",
      "Learning Rate: 0.001, Epoch: 12, Test Loss: 0.3686, Validation Loss: 0.3604\n",
      "Learning Rate: 0.001, Epoch: 13, Test Loss: 0.3675, Validation Loss: 0.3617\n",
      "Learning Rate: 0.001, Epoch: 14, Test Loss: 0.3673, Validation Loss: 0.3608\n",
      "Learning Rate: 0.001, Epoch: 15, Test Loss: 0.3674, Validation Loss: 0.3586\n",
      "Learning Rate: 0.0005, Epoch: 1, Test Loss: 0.3651, Validation Loss: 0.3596\n",
      "Learning Rate: 0.0005, Epoch: 2, Test Loss: 0.3647, Validation Loss: 0.3586\n",
      "Learning Rate: 0.0005, Epoch: 3, Test Loss: 0.3641, Validation Loss: 0.3574\n",
      "Learning Rate: 0.0005, Epoch: 4, Test Loss: 0.3649, Validation Loss: 0.3567\n",
      "Learning Rate: 0.0005, Epoch: 5, Test Loss: 0.3640, Validation Loss: 0.3546\n",
      "Learning Rate: 0.0005, Epoch: 6, Test Loss: 0.3628, Validation Loss: 0.3535\n",
      "Learning Rate: 0.0005, Epoch: 7, Test Loss: 0.3624, Validation Loss: 0.3554\n",
      "Learning Rate: 0.0005, Epoch: 8, Test Loss: 0.3623, Validation Loss: 0.3537\n",
      "Learning Rate: 0.0005, Epoch: 9, Test Loss: 0.3631, Validation Loss: 0.3545\n",
      "Learning Rate: 0.0005, Epoch: 10, Test Loss: 0.3625, Validation Loss: 0.3518\n",
      "Learning Rate: 0.0005, Epoch: 11, Test Loss: 0.3609, Validation Loss: 0.3539\n",
      "Learning Rate: 0.0005, Epoch: 12, Test Loss: 0.3606, Validation Loss: 0.3507\n",
      "Learning Rate: 0.0005, Epoch: 13, Test Loss: 0.3609, Validation Loss: 0.3513\n",
      "Learning Rate: 0.0005, Epoch: 14, Test Loss: 0.3595, Validation Loss: 0.3533\n",
      "Learning Rate: 0.0005, Epoch: 15, Test Loss: 0.3616, Validation Loss: 0.3531\n"
     ]
    }
   ],
   "source": [
    "# Paths for saving\n",
    "save_dir = \"models/alexnet_variation_BCELoss\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# Define Params\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs = 15\n",
    "learning_rates = [0.001, 0.0005]\n",
    "custom_alexnet.to(device)\n",
    "\n",
    "# Track losses for visualization\n",
    "train_losses_dict = {}\n",
    "valid_losses_dict = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    optimizer = optim.Adam(custom_alexnet.parameters(), lr=lr)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []  \n",
    "    valid_losses = []  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        avg_train_loss = train(custom_alexnet, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        valid_loss = evaluate(custom_alexnet, valid_loader, criterion, device)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Print validation loss\n",
    "        print(f'Learning Rate: {lr}, Epoch: {epoch+1}, Test Loss: {avg_train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "        \n",
    "        # Save the best model if validation loss improves\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(custom_alexnet.state_dict(), os.path.join(save_dir, f'best_model_lr_{lr}.pt'))\n",
    "\n",
    "    # Store losses for visualization\n",
    "    train_losses_dict[lr] = train_losses\n",
    "    valid_losses_dict[lr] = valid_losses\n",
    "\n",
    "# Save losses dictionaries for visualization later\n",
    "torch.save(train_losses_dict, os.path.join(save_dir, 'train_losses.pt'))\n",
    "torch.save(valid_losses_dict, os.path.join(save_dir, 'valid_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

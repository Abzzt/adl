{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4620 validated image filenames belonging to 15 classes.\n",
      "Found 1155 validated image filenames belonging to 15 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abigail/Documents/adl/data_proccessing.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_new = pd.concat([df_new, data_entry[data_entry[l]==1][:500]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.02745098 0.02745098 0.02745098]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.14901961 0.14901961 0.14901961]\n",
      "   [0.15686275 0.15686275 0.15686275]]\n",
      "\n",
      "  [[0.03137255 0.03137255 0.03137255]\n",
      "   [0.04705883 0.04705883 0.04705883]\n",
      "   [0.03529412 0.03529412 0.03529412]\n",
      "   ...\n",
      "   [0.18431373 0.18431373 0.18431373]\n",
      "   [0.17254902 0.17254902 0.17254902]\n",
      "   [0.16862746 0.16862746 0.16862746]]\n",
      "\n",
      "  [[0.02745098 0.02745098 0.02745098]\n",
      "   [0.02745098 0.02745098 0.02745098]\n",
      "   [0.03529412 0.03529412 0.03529412]\n",
      "   ...\n",
      "   [0.18039216 0.18039216 0.18039216]\n",
      "   [0.16862746 0.16862746 0.16862746]\n",
      "   [0.1764706  0.1764706  0.1764706 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6156863  0.6156863  0.6156863 ]\n",
      "   [0.61960787 0.61960787 0.61960787]\n",
      "   [0.6745098  0.6745098  0.6745098 ]\n",
      "   ...\n",
      "   [0.7490196  0.7490196  0.7490196 ]\n",
      "   [0.73333335 0.73333335 0.73333335]\n",
      "   [0.7372549  0.7372549  0.7372549 ]]\n",
      "\n",
      "  [[0.6156863  0.6156863  0.6156863 ]\n",
      "   [0.627451   0.627451   0.627451  ]\n",
      "   [0.7254902  0.7254902  0.7254902 ]\n",
      "   ...\n",
      "   [0.7490196  0.7490196  0.7490196 ]\n",
      "   [0.7372549  0.7372549  0.7372549 ]\n",
      "   [0.7411765  0.7411765  0.7411765 ]]\n",
      "\n",
      "  [[0.6156863  0.6156863  0.6156863 ]\n",
      "   [0.62352943 0.62352943 0.62352943]\n",
      "   [0.7372549  0.7372549  0.7372549 ]\n",
      "   ...\n",
      "   [0.7490196  0.7490196  0.7490196 ]\n",
      "   [0.7411765  0.7411765  0.7411765 ]\n",
      "   [0.7372549  0.7372549  0.7372549 ]]]\n",
      "\n",
      "\n",
      " [[[0.02352941 0.02352941 0.02352941]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]]\n",
      "\n",
      "  [[0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.02352941 0.02352941 0.02352941]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]]\n",
      "\n",
      "  [[0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6509804  0.6509804  0.6509804 ]\n",
      "   [0.7019608  0.7019608  0.7019608 ]\n",
      "   [0.69411767 0.69411767 0.69411767]\n",
      "   ...\n",
      "   [0.1137255  0.1137255  0.1137255 ]\n",
      "   [0.13725491 0.13725491 0.13725491]\n",
      "   [0.15294118 0.15294118 0.15294118]]\n",
      "\n",
      "  [[0.65882355 0.65882355 0.65882355]\n",
      "   [0.70980394 0.70980394 0.70980394]\n",
      "   [0.69803923 0.69803923 0.69803923]\n",
      "   ...\n",
      "   [0.10980393 0.10980393 0.10980393]\n",
      "   [0.12941177 0.12941177 0.12941177]\n",
      "   [0.14117648 0.14117648 0.14117648]]\n",
      "\n",
      "  [[0.7294118  0.7294118  0.7294118 ]\n",
      "   [0.7176471  0.7176471  0.7176471 ]\n",
      "   [0.72156864 0.72156864 0.72156864]\n",
      "   ...\n",
      "   [0.10980393 0.10980393 0.10980393]\n",
      "   [0.1137255  0.1137255  0.1137255 ]\n",
      "   [0.13725491 0.13725491 0.13725491]]]\n",
      "\n",
      "\n",
      " [[[0.06666667 0.06666667 0.06666667]\n",
      "   [0.05490196 0.05490196 0.05490196]\n",
      "   [0.04313726 0.04313726 0.04313726]\n",
      "   ...\n",
      "   [0.09803922 0.09803922 0.09803922]\n",
      "   [0.10196079 0.10196079 0.10196079]\n",
      "   [0.10196079 0.10196079 0.10196079]]\n",
      "\n",
      "  [[0.06666667 0.06666667 0.06666667]\n",
      "   [0.0509804  0.0509804  0.0509804 ]\n",
      "   [0.04313726 0.04313726 0.04313726]\n",
      "   ...\n",
      "   [0.08627451 0.08627451 0.08627451]\n",
      "   [0.08627451 0.08627451 0.08627451]\n",
      "   [0.09411766 0.09411766 0.09411766]]\n",
      "\n",
      "  [[0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0509804  0.0509804  0.0509804 ]\n",
      "   [0.04313726 0.04313726 0.04313726]\n",
      "   ...\n",
      "   [0.07843138 0.07843138 0.07843138]\n",
      "   [0.07843138 0.07843138 0.07843138]\n",
      "   [0.08627451 0.08627451 0.08627451]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.08235294 0.08235294 0.08235294]\n",
      "   [0.06666667 0.06666667 0.06666667]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   ...\n",
      "   [0.3921569  0.3921569  0.3921569 ]\n",
      "   [0.27450982 0.27450982 0.27450982]\n",
      "   [0.10588236 0.10588236 0.10588236]]\n",
      "\n",
      "  [[0.08235294 0.08235294 0.08235294]\n",
      "   [0.07058824 0.07058824 0.07058824]\n",
      "   [0.06666667 0.06666667 0.06666667]\n",
      "   ...\n",
      "   [0.4039216  0.4039216  0.4039216 ]\n",
      "   [0.29411766 0.29411766 0.29411766]\n",
      "   [0.1137255  0.1137255  0.1137255 ]]\n",
      "\n",
      "  [[0.10196079 0.10196079 0.10196079]\n",
      "   [0.08235294 0.08235294 0.08235294]\n",
      "   [0.07843138 0.07843138 0.07843138]\n",
      "   ...\n",
      "   [0.41176474 0.41176474 0.41176474]\n",
      "   [0.29803923 0.29803923 0.29803923]\n",
      "   [0.10980393 0.10980393 0.10980393]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]]\n",
      "\n",
      "\n",
      " [[[0.1764706  0.1764706  0.1764706 ]\n",
      "   [0.17254902 0.17254902 0.17254902]\n",
      "   [0.16862746 0.16862746 0.16862746]\n",
      "   ...\n",
      "   [0.83921576 0.83921576 0.83921576]\n",
      "   [0.8431373  0.8431373  0.8431373 ]\n",
      "   [0.8352942  0.8352942  0.8352942 ]]\n",
      "\n",
      "  [[0.12156864 0.12156864 0.12156864]\n",
      "   [0.14901961 0.14901961 0.14901961]\n",
      "   [0.1254902  0.1254902  0.1254902 ]\n",
      "   ...\n",
      "   [0.8313726  0.8313726  0.8313726 ]\n",
      "   [0.8313726  0.8313726  0.8313726 ]\n",
      "   [0.8313726  0.8313726  0.8313726 ]]\n",
      "\n",
      "  [[0.10588236 0.10588236 0.10588236]\n",
      "   [0.10588236 0.10588236 0.10588236]\n",
      "   [0.09803922 0.09803922 0.09803922]\n",
      "   ...\n",
      "   [0.8235295  0.8235295  0.8235295 ]\n",
      "   [0.8196079  0.8196079  0.8196079 ]\n",
      "   [0.82745105 0.82745105 0.82745105]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.18823531 0.18823531 0.18823531]\n",
      "   [0.23529413 0.23529413 0.23529413]\n",
      "   [0.32156864 0.32156864 0.32156864]\n",
      "   ...\n",
      "   [0.8705883  0.8705883  0.8705883 ]\n",
      "   [0.86666673 0.86666673 0.86666673]\n",
      "   [0.86666673 0.86666673 0.86666673]]\n",
      "\n",
      "  [[0.21176472 0.21176472 0.21176472]\n",
      "   [0.25882354 0.25882354 0.25882354]\n",
      "   [0.34117648 0.34117648 0.34117648]\n",
      "   ...\n",
      "   [0.86666673 0.86666673 0.86666673]\n",
      "   [0.8705883  0.8705883  0.8705883 ]\n",
      "   [0.86666673 0.86666673 0.86666673]]\n",
      "\n",
      "  [[0.23137257 0.23137257 0.23137257]\n",
      "   [0.2784314  0.2784314  0.2784314 ]\n",
      "   [0.3647059  0.3647059  0.3647059 ]\n",
      "   ...\n",
      "   [0.86666673 0.86666673 0.86666673]\n",
      "   [0.8705883  0.8705883  0.8705883 ]\n",
      "   [0.86274517 0.86274517 0.86274517]]]\n",
      "\n",
      "\n",
      " [[[0.1137255  0.1137255  0.1137255 ]\n",
      "   [0.13333334 0.13333334 0.13333334]\n",
      "   [0.16862746 0.16862746 0.16862746]\n",
      "   ...\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   [0.03921569 0.03921569 0.03921569]\n",
      "   [0.01568628 0.01568628 0.01568628]]\n",
      "\n",
      "  [[0.1254902  0.1254902  0.1254902 ]\n",
      "   [0.14901961 0.14901961 0.14901961]\n",
      "   [0.19215688 0.19215688 0.19215688]\n",
      "   ...\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   [0.01568628 0.01568628 0.01568628]\n",
      "   [0.01568628 0.01568628 0.01568628]]\n",
      "\n",
      "  [[0.13333334 0.13333334 0.13333334]\n",
      "   [0.1764706  0.1764706  0.1764706 ]\n",
      "   [0.21176472 0.21176472 0.21176472]\n",
      "   ...\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   [0.01568628 0.01568628 0.01568628]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8352942  0.8352942  0.8352942 ]\n",
      "   [0.83921576 0.83921576 0.83921576]\n",
      "   [0.8352942  0.8352942  0.8352942 ]\n",
      "   ...\n",
      "   [0.7294118  0.7294118  0.7294118 ]\n",
      "   [0.72156864 0.72156864 0.72156864]\n",
      "   [0.7254902  0.7254902  0.7254902 ]]\n",
      "\n",
      "  [[0.8352942  0.8352942  0.8352942 ]\n",
      "   [0.8431373  0.8431373  0.8431373 ]\n",
      "   [0.8352942  0.8352942  0.8352942 ]\n",
      "   ...\n",
      "   [0.7294118  0.7294118  0.7294118 ]\n",
      "   [0.7294118  0.7294118  0.7294118 ]\n",
      "   [0.7411765  0.7411765  0.7411765 ]]\n",
      "\n",
      "  [[0.83921576 0.83921576 0.83921576]\n",
      "   [0.83921576 0.83921576 0.83921576]\n",
      "   [0.8431373  0.8431373  0.8431373 ]\n",
      "   ...\n",
      "   [0.7411765  0.7411765  0.7411765 ]\n",
      "   [0.7411765  0.7411765  0.7411765 ]\n",
      "   [0.7372549  0.7372549  0.7372549 ]]]] [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from data_proccessing import image_size, labels\n",
    "import tensorflow as tf\n",
    "\n",
    "dnet = DenseNet121(\n",
    "        input_shape=(image_size, image_size, 3),\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "dnet.trainable = True\n",
    "\n",
    "dnet_model = tf.keras.Sequential([\n",
    "    dnet,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(len(labels), activation='sigmoid')])\n",
    "\n",
    "dnet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics=['AUC','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abigail/Documents/adl/venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 4s/step - AUC: 0.6062 - Precision: 0.1347 - Recall: 0.0509 - loss: 0.3946 - val_AUC: 0.5513 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_loss: 0.3717\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 4s/step - AUC: 0.6869 - Precision: 0.4437 - Recall: 0.0121 - loss: 0.3284 - val_AUC: 0.5643 - val_Precision: 0.1399 - val_Recall: 0.0212 - val_loss: 0.3964\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 4s/step - AUC: 0.7077 - Precision: 0.5141 - Recall: 0.0346 - loss: 0.3176 - val_AUC: 0.5959 - val_Precision: 0.1489 - val_Recall: 0.1248 - val_loss: 0.4631\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 4s/step - AUC: 0.7281 - Precision: 0.5685 - Recall: 0.0503 - loss: 0.3134 - val_AUC: 0.6420 - val_Precision: 0.4175 - val_Recall: 0.0223 - val_loss: 0.3476\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 4s/step - AUC: 0.7429 - Precision: 0.5748 - Recall: 0.0567 - loss: 0.3083 - val_AUC: 0.6838 - val_Precision: 0.4922 - val_Recall: 0.0326 - val_loss: 0.3339\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 4s/step - AUC: 0.7517 - Precision: 0.5193 - Recall: 0.0722 - loss: 0.3031 - val_AUC: 0.6016 - val_Precision: 0.0919 - val_Recall: 0.0176 - val_loss: 0.3828\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 4s/step - AUC: 0.7589 - Precision: 0.5569 - Recall: 0.0771 - loss: 0.3020 - val_AUC: 0.6165 - val_Precision: 0.2838 - val_Recall: 0.0109 - val_loss: 0.3558\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 18:39:56.478641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 3 of 8\n",
      "2024-04-14 18:39:59.141786: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m809s\u001b[0m 4s/step - AUC: 0.7635 - Precision: 0.5545 - Recall: 0.0726 - loss: 0.2989 - val_AUC: 0.5777 - val_Precision: 0.1571 - val_Recall: 0.0171 - val_loss: 0.3850\n",
      "Epoch 9/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 4s/step - AUC: 0.7678 - Precision: 0.6064 - Recall: 0.0959 - loss: 0.2992 - val_AUC: 0.6932 - val_Precision: 0.6857 - val_Recall: 0.0124 - val_loss: 0.3257\n",
      "Epoch 10/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2452s\u001b[0m 17s/step - AUC: 0.7791 - Precision: 0.6271 - Recall: 0.1104 - loss: 0.2946 - val_AUC: 0.6976 - val_Precision: 0.5735 - val_Recall: 0.0202 - val_loss: 0.3249\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 754ms/step\n"
     ]
    }
   ],
   "source": [
    "from data_proccessing import train_gen, valid_gen, X_val, y_val\n",
    "\n",
    "history = dnet_model.fit(train_gen,\n",
    "              validation_data = valid_gen,\n",
    "              epochs = 10,)\n",
    "\n",
    "y_pred = dnet_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atelectasis: actual: 16.54%, predicted: 18.59%\n",
      "Cardiomegaly: actual: 9.09%, predicted: 6.88%\n",
      "Consolidation: actual: 9.78%, predicted: 12.95%\n",
      "Edema: actual: 9.44%, predicted: 4.35%\n",
      "Effusion: actual: 17.66%, predicted: 14.76%\n",
      "Emphysema: actual: 9.09%, predicted: 8.40%\n",
      "Fibrosis: actual: 8.92%, predicted: 7.82%\n",
      "Hernia: actual: 3.98%, predicted: 10.18%\n",
      "Infiltration: actual: 25.54%, predicted: 15.74%\n",
      "Mass: actual: 9.96%, predicted: 18.48%\n",
      "No Finding: actual: 8.66%, predicted: 6.00%\n",
      "Nodule: actual: 10.39%, predicted: 11.00%\n",
      "Pleural_Thickening: actual: 8.40%, predicted: 9.52%\n",
      "Pneumonia: actual: 8.92%, predicted: 6.51%\n",
      "Pneumothorax: actual: 10.82%, predicted: 8.67%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for label, p_count, t_count in zip(labels,\n",
    "                                     100 * np.mean(y_pred, 0),\n",
    "                                     100 * np.mean(y_val, 0)):\n",
    "    print('%s: actual: %2.2f%%, predicted: %2.2f%%' % (label, t_count, p_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42s/step - AUC: 0.6976 - Precision: 0.5735 - Recall: 0.0202 - loss: 0.3249\n",
      "AUC: 0.32492390275001526\n",
      "Precision: 0.6976433396339417\n",
      "Recall: 0.5735294222831726\n",
      "Loss 0.5735294222831726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_results = dnet_model.evaluate(valid_gen)\n",
    "print(\"AUC:\", evaluation_results[0])\n",
    "print(\"Precision:\", evaluation_results[1])\n",
    "print(\"Recall:\", evaluation_results[2])\n",
    "print(\"Loss\", evaluation_results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet_model.save('models/densNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

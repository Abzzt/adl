{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\adl\\src\\data_processing.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_new = pd.concat([df_new, temp_df])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the path to the src directory\n",
    "src_path = os.path.abspath('../src')\n",
    "\n",
    "# Check if the src directory exists\n",
    "if src_path not in sys.path:\n",
    "    # Append src directory to the Python path\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from data_processing import valid_loader, train_loader, test_loader, labels\n",
    "from helper import train, evaluate, test, visualise_all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dense Layer\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, growth_rate*4, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(growth_rate*4)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(growth_rate*4, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.classifier = nn.Linear(growth_rate, len(labels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu1(self.norm1(x)))\n",
    "        out = self.conv2(self.relu2(self.norm2(out)))\n",
    "        out = torch.cat([x, out], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dense Block\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([_DenseLayer(in_channels + i * growth_rate, growth_rate) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transition Block\n",
    "class _Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool(self.conv(self.relu(self.norm(x))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDenseNet(\n",
      "  (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu0): ReLU(inplace=True)\n",
      "  (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (denseblock1): _DenseBlock(\n",
      "    (layers): ModuleList(\n",
      "      (0): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (classifier): Linear(in_features=4, out_features=15, bias=True)\n",
      "      )\n",
      "      (1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(68, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (classifier): Linear(in_features=4, out_features=15, bias=True)\n",
      "      )\n",
      "      (2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(72, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (classifier): Linear(in_features=4, out_features=15, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transition1): _Transition(\n",
      "    (norm): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv): Conv2d(76, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (denseblock2): _DenseBlock(\n",
      "    (layers): ModuleList(\n",
      "      (0): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (classifier): Linear(in_features=4, out_features=15, bias=True)\n",
      "      )\n",
      "      (1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(132, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (classifier): Linear(in_features=4, out_features=15, bias=True)\n",
      "      )\n",
      "      (2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(136, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (classifier): Linear(in_features=4, out_features=15, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm5): BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (linear): Linear(in_features=140, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define Custom DenseNet model\n",
    "class CustomDenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomDenseNet, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False).to(torch.float32)\n",
    "        self.norm0 = nn.BatchNorm2d(64)\n",
    "        self.relu0 = nn.ReLU(inplace=True)\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.denseblock1 = _DenseBlock(num_layers=3, in_channels=64, growth_rate=4)  # Reduced growth rate\n",
    "        self.transition1 = _Transition(in_channels=76, out_channels=128)\n",
    "        self.denseblock2 = _DenseBlock(num_layers=3, in_channels=128, growth_rate=4)  # Added second dense block\n",
    "        self.norm5 = nn.BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Adjusted to the output size of the last dense block\n",
    "        self.pool1 = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.linear = nn.Linear(in_features=(140), out_features=15, bias=True)  # Adjusted input features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.norm0(x)\n",
    "        x = self.relu0(x)\n",
    "        out = self.pool0(x)\n",
    "        out = self.denseblock1(out)\n",
    "        out = self.transition1(out)\n",
    "        out = self.denseblock2(out)\n",
    "        out = F.relu(self.norm5(out))\n",
    "        out = self.pool1(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "# Create an instance of Custom DenseNet\n",
    "custom_densenet = CustomDenseNet()\n",
    "custom_densenet.to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(custom_densenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Paths for saving\n",
    "save_dir = \"../models/densenet_variation_15e/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define Params\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs = 15\n",
    "learning_rates = [0.001, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track losses for visualization\n",
    "train_losses_dict = {}\n",
    "valid_losses_dict = {}\n",
    "\n",
    "# Iterate over different learning rates\n",
    "for lr in learning_rates:\n",
    "    optimizer = optim.Adam(custom_densenet.parameters(), lr=lr)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []  \n",
    "    valid_losses = []  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        avg_train_loss = train(custom_densenet, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        valid_loss = evaluate(custom_densenet, valid_loader, criterion, device)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Print validation loss\n",
    "        print(f'Learning Rate: {lr}, Epoch: {epoch+1}, Test Loss: {avg_train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "        \n",
    "        # Save the best model if validation loss improves\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(custom_densenet.state_dict(), os.path.join(save_dir, f'best_model_lr_{lr}.pt'))\n",
    "\n",
    "    # Store losses for visualization\n",
    "    train_losses_dict[lr] = train_losses\n",
    "    valid_losses_dict[lr] = valid_losses\n",
    "\n",
    "# Save losses dictionaries for visualization later\n",
    "torch.save(train_losses_dict, os.path.join(save_dir, 'train_losses.pt'))\n",
    "torch.save(valid_losses_dict, os.path.join(save_dir, 'valid_losses.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision  Accuracy  Recall  F1-score\n",
      "0         0.0  0.811512     0.0         0\n",
      "1         0.0  0.883747     0.0         0\n",
      "2         0.0  0.878104     0.0         0\n",
      "3         0.0  0.892777     0.0         0\n",
      "4         0.0  0.762980     0.0         0\n",
      "5         0.0  0.917607     0.0         0\n",
      "6         0.0  0.898420     0.0         0\n",
      "7         0.0  0.945824     0.0         0\n",
      "8         0.0  0.713318     0.0         0\n",
      "9         0.0  0.889391     0.0         0\n",
      "10        0.0  0.932280     0.0         0\n",
      "11        0.0  0.881490     0.0         0\n",
      "12        0.0  0.879233     0.0         0\n",
      "13        0.0  0.896163     0.0         0\n",
      "14        0.0  0.889391     0.0         0 0.3668302388063499\n",
      "    Precision  Accuracy  Recall  F1-score\n",
      "0         0.0  0.811512     0.0         0\n",
      "1         0.0  0.883747     0.0         0\n",
      "2         0.0  0.878104     0.0         0\n",
      "3         0.0  0.892777     0.0         0\n",
      "4         0.0  0.762980     0.0         0\n",
      "5         0.0  0.917607     0.0         0\n",
      "6         0.0  0.898420     0.0         0\n",
      "7         0.0  0.945824     0.0         0\n",
      "8         0.0  0.713318     0.0         0\n",
      "9         0.0  0.889391     0.0         0\n",
      "10        0.0  0.932280     0.0         0\n",
      "11        0.0  0.881490     0.0         0\n",
      "12        0.0  0.879233     0.0         0\n",
      "13        0.0  0.896163     0.0         0\n",
      "14        0.0  0.889391     0.0         0 0.3592791126242706\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "for lr in learning_rates:\n",
    "    model_path = os.path.join(save_dir, f'best_model_lr_{lr}.pt')  # Change the filename accordingly\n",
    "    custom_densenet.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    metrics, loss = test(custom_densenet, test_loader, criterion, device, num_classes)\n",
    "    print(metrics, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data at index 0: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 1: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 2: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 3: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 4: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 5: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 6: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 7: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 8: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 9: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 10: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 11: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 12: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 13: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 14: 'NoneType' object has no attribute 'read'\n",
      "Error loading data at index 15: 'NoneType' object has no attribute 'read'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Change the filename accordingly\u001b[39;00m\n\u001b[0;32m      4\u001b[0m custom_densenet\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device)))\n\u001b[1;32m----> 5\u001b[0m metrics, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_densenet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics, loss)\n",
      "File \u001b[1;32md:\\adl\\src\\helper.py:53\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, criterion, device, num_classes)\u001b[0m\n\u001b[0;32m     50\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     54\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     55\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mandi\\anaconda3\\envs\\adl\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:152\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "for lr in learning_rates:\n",
    "    model_path = os.path.join(save_dir, f'best_model_lr_{lr}.pt')  # Change the filename accordingly\n",
    "    custom_densenet.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    metrics, loss = test(custom_densenet, test_loader, criterion, device, 14)\n",
    "    print(metrics, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train Loss and Validation Loss on the same graph\n",
    "train_model_path = os.path.join(save_dir, f'train_losses.pt')\n",
    "valid_model_path = os.path.join(save_dir, f'valid_losses.pt')\n",
    "\n",
    "visualise_all_loss(train_model_path, valid_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
